{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cabde846",
   "metadata": {},
   "source": [
    "# Multi-units order analysis for Fall 2021\n",
    "Comments from Linda: I'm most interested in finding the multi unit orders from last Fall so we set ourselves up for success this coming season. \n",
    "I can't wait until you stitch together Q3,Oct,Nov,Dec files..\n",
    "\n",
    "Original Location and csv files: S:\\Olivia\\Txn Data Extracts from Akshay  \n",
    "1. Q3_2021_NEW_LOGIC_USING_UNITS_REFRESHED_MDR_SORTED.csv\n",
    "2. OCTOBER_2021_NEW_LOGIC_USING_UNITS_REFRESHED_MDR_SORTED.csv\n",
    "3. NOVEMBER_2021_NEW_LOGIC_USING_UNITS_REFRESHED_MDR_SORTED.csv\n",
    "4. DECEMBER_2021_NEW_LOGIC_USING_UNITS_REFRESHED_MDR_SORTED.csv\n",
    "\n",
    "To my directory: K:\\Logistics\\Co-op\\Yongpeng\\2021 Transaction Data from Olivia Q3 and Fall\n",
    "\n",
    "## Ecomm – Make better decision\n",
    "1. Top styles in each province so we can increase safety stock\n",
    "2. Association (Based on the order # and not by split shipment, can you tell me what products are being ordered together so that we can co-locate the product in the same store so that all future orders can be shipped from a single location)\n",
    "3. Prediction of selling curve\n",
    "\n",
    ">comments from Linda:\n",
    "\n",
    ">Here’s a list of questions I’d like you to solve: \n",
    ">1.\t What are the top styles in each province so we can increase safety stock…might be top items that cover 50% of the ecomm sales or a list of the top x# of styles.\n",
    ">2.\tBased on the order # and not by split shipment, can you tell me what products are being ordered together so that we can co-locate the product in the same store so that all future orders can be shipped from a single location.\n",
    ">3.\tCan you do a similar exercise that Olivia did for Spring and provide a list of styles that were ordered 3 or more units in the same order.  I’m looking to increase depth in the styles and answer if there are small businesses ordering multiple items.\n",
    ">4.\tIf we get to this one, I might give you a fringe size sku list for Menswear to see if you can pull some insights.\n",
    "\n",
    ">Comments from Linda for question 1: You would need to tally up total quantity sales by province, rank the styles and look for a good cutoff point.\n",
    "75% cut off might be too much...it might equal a crazy 5000 styles...I'm just making this up...I have no idea.\n",
    "See what the results are first and I can help determine cut off with you. You'll probably end up choosing a % cut off or # of styles cutoff.\n",
    "\n",
    ">My comment:\n",
    "I will move ahead with this approach and generate a distribution for all the styles in each province, \n",
    "and then extract those that have 70% (this can be changed) and to see if it is feasible or not\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a5f5e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d658816a",
   "metadata": {},
   "source": [
    "### Question 1: Find the top styles in each province so we can increase **safety stock**.   \n",
    "You want find a threshold for each province so that a minimum list of styles can represent the majority of the items sold.\n",
    "\n",
    "#### Some Clarification from Olivia (August 9 2022 using VM)\n",
    "Good morning Olivia, thanks for the advice to only look at the Ecomm data. And after I merging dataset together, I noticed another 2 questions as attached. The 1st one is some negative number in the ITEM_QTY column. Why is that? Do I need to take them away? The 2nd question is the Store State/Prov and FULFIL_STORE_PROVINCE not the same. Yesterday, you mentioned that if FULFIL_STORE_PROVINCE is missing, I can use STORE_NUM to find out which province it belongs to. However, I think they are 2 different things, are they not?\n",
    "![Question](./Image/Question.png)\n",
    "\n",
    "Comments from Olivia:\n",
    "\n",
    "\n",
    "So, negative quantities are returns txn . 344 is an online store (all web/tablet orders were 344). Fulfilled Store Prov for those returns txn would not be relevant anymore. If you want to know what the Initial fulfilling store number is, there's an attribute called \"ORIG_FULFILL_STORE\" or something like that in the dataset.\n",
    "\n",
    "Ok, that makes sense. So what is difference between Store State/Prov and FULFIL_STORE_PROVINCE, besides what you mentioned 344 online store. My thought is: Store State/Prov is where this transaction is triggered, and FULFIL_STORE_PROVINCE is where this particular transaction is fulfiled?\n",
    "\n",
    "If it's a sales txn, the two store prov should be the same. If it's a returns txn, the STORE_PROV would be where the person returning the product (i.e. mailing it back would goes to 344 and corp office prov is in Alberta. If they are returning it to store, then it should have the returning store# and the store Prov). I don't use Fulfilled_Store_Prov for returns. Again, check ORIG_FULFILL_STORE_PROV to be the safest or you can compare the two if there's any difference\n",
    "\n",
    "Thank you Olivia. Ok, I want to give you a full picture of what I am doing. Attached is the whole features of this dataset, and red label is what I pulled out for now. And then the question I am looking after for is a top list of STYLEs that are most frequently purchased in different province.\n",
    "![Full_Features](./Image/Full_Features.png)\n",
    "\n",
    "\n",
    "In my opinion, you can exclude the Returns txn. I think the question you are trying to answer is based on sales only. To exclude returns, you can look into NORM SUB CHANNEL and filter on \"GROSS WEB SALES\". This way, you can just look at STORE_NUM without the ORIG_SHIP blah blah blah. Does that make sense?\n",
    "\n",
    "Yes, you are right. I only need to look at sales only. So I put them in a summary: 1, because I only need to look at E-comm, I will filter on \"`Norm Channel` == 'E-Comm'\". 2, To exclude returns, I will look into NORM SUB CHANNEL and filter on \"GROSS WEB SALES\". 3, after these two filters, I will only pull the features for ['SALES_DATE', 'Transaction Number', 'STORE_NUM', 'Store State/Prov', 'Norm Channel', 'NORM SUB CHANNEL', 'SKU_NUM','ITEM_QTY', 'STYLE_NO']. I dont need ORIG_FULFILL_STORE_PROV because after these two filters, 'STORE_NUM' & 'Store State/Prov' will be the same as ORIG_FULFILL_STORE_PROV, right?\n",
    "\n",
    "Yes, and to make your life even easier, filter on \"Norm Sub Channel\" is enough. The Gross Web Sales will take care of the \"Norm Channel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d7a2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list_reduced = ['SALES_DATE', 'Transaction Number', 'ORDER_NUMBER', 'FULFILL_STORE', 'Selling Cycle', 'Norm Sub Channel',\n",
    "                    'SKU_NUM', 'SKU_NAME', 'STYLE_NO', 'STYLE_NAME', 'COMMODITY_NO',\n",
    "                    'COMMODITY_NAME', 'ITEM_QTY', 'SHIPPED_TO_POSTALCODE', 'SHIPPED_TO_CITY', 'SHIPPED_TO_PROVINCE']\n",
    "dtypes = {\"Transaction Number\":\"str\", 'ORDER_NUMBER':'str', 'FULFILL_STORE':'str', 'Selling Cycle':'str', 'Norm Sub Channel':\"str\", \n",
    "          \"SKU_NUM\":\"str\", 'SKU_NAME':\"str\", \"STYLE_NO\":\"str\", 'STYLE_NAME':'str', 'COMMODITY_NO':'str',\n",
    "          'COMMODITY_NAME':'str', \"ITEM_QTY\":\"int\",\n",
    "         'SHIPPED_TO_POSTALCODE':\"str\", 'SHIPPED_TO_CITY':\"str\", 'SHIPPED_TO_PROVINCE':\"str\"}\n",
    "#read in the csv\n",
    "# OCTOBER_2021_reduced_web_ecomm_pre = pd.read_csv(r\"S:/Olivia/Txn Data Extracts from Akshay/OCTOBER_2021_NEW_LOGIC_USING_UNITS_REFRESHED_MDR_SORTED.csv\",\n",
    "#                           usecols = col_list_reduced, engine = 'c',dtype=dtypes, parse_dates = [\"SALES_DATE\"], encoding='latin1', chunksize = 10000000)\n",
    "# OCTOBER_2021_reduced_web_ecomm_final = pd.concat([chunk.query(\"`Norm Sub Channel` == 'GROSS WEB SALES'\") for chunk in OCTOBER_2021_reduced_web_ecomm_pre], ignore_index = True)\n",
    "\n",
    "# NOVEMBER_2021_reduced_web_ecomm_pre = pd.read_csv(r\"S:/Olivia/Txn Data Extracts from Akshay/NOVEMBER_2021_NEW_LOGIC_USING_UNITS_REFRESHED_MDR_SORTED.csv\",\n",
    "#                           usecols = col_list_reduced, engine = 'c',dtype=dtypes, parse_dates = [\"SALES_DATE\"],encoding='latin1',chunksize = 10000000)\n",
    "# NOVEMBER_2021_reduced_web_ecomm_final = pd.concat([chunk.query(\"`Norm Sub Channel` == 'GROSS WEB SALES'\") for chunk in NOVEMBER_2021_reduced_web_ecomm_pre], ignore_index = True)\n",
    "\n",
    "# DECEMBER_2021_reduced_web_ecomm_pre = pd.read_csv(r\"S:/Olivia/Txn Data Extracts from Akshay/DECEMBER_2021_NEW_LOGIC_USING_UNITS_REFRESHED_MDR_SORTED.csv\",\n",
    "#                           usecols = col_list_reduced, engine = 'c',dtype=dtypes, parse_dates = [\"SALES_DATE\"],encoding='latin1',chunksize = 10000000)\n",
    "# DECEMBER_2021_reduced_web_ecomm_final = pd.concat([chunk.query(\"`Norm Sub Channel` == 'GROSS WEB SALES'\") for chunk in DECEMBER_2021_reduced_web_ecomm_pre], ignore_index = True)\n",
    "\n",
    "# Q3_2021_reduced_web_ecomm_pre = pd.read_csv(r\"S:/Olivia/Txn Data Extracts from Akshay/Q3_2021_NEW_LOGIC_USING_UNITS_REFRESHED_MDR_SORTED.csv\",\n",
    "#                           usecols = col_list_reduced, engine = 'c',dtype=dtypes, parse_dates = [\"SALES_DATE\"],encoding='latin1',chunksize = 10000000)\n",
    "# Q3_2021_reduced_web_ecomm_final = pd.concat([chunk.query(\"`Norm Sub Channel` == 'GROSS WEB SALES'\") for chunk in Q3_2021_reduced_web_ecomm_pre], ignore_index = True)\n",
    "\n",
    "# Fall_2021_reduced_web_ecomm_final = pd.concat([Q3_2021_reduced_web_ecomm_final, OCTOBER_2021_reduced_web_ecomm_final, NOVEMBER_2021_reduced_web_ecomm_final, DECEMBER_2021_reduced_web_ecomm_final])\n",
    "# Fall_2021_reduced_web_ecomm_final.to_csv(r\"S:\\Olivia\\Txn Data Extracts from Akshay\\Yongpeng Fu August 9 2022\\Fall_2021_reduced_web_ecomm_final.csv\", index = False)\n",
    "#Since I have saved the results, I will just load them directly.\n",
    "Fall_2021_reduced_web_ecomm_final = pd.read_csv(r\"K:\\Logistics\\Co-op\\Yongpeng\\2021 Transaction Data from Olivia Q3 and Fall\\Yongpeng Fu August 9 2022\\Fall_2021_reduced_web_ecomm_final.csv\",\n",
    "                                               dtype=dtypes, parse_dates = [\"SALES_DATE\"])\n",
    "\n",
    "#So, negative quantities are returns txn . 344 is an online store (all web/tablet orders were 344). \n",
    "#Fulfilled Store Prov for those returns txn would not be relevant anymore. I will filter out any rows that have < 0 quantity\n",
    "Fall_2021_reduced_ecomm_final_Item_pos = Fall_2021_reduced_web_ecomm_final.query(\"ITEM_QTY > 0\")\n",
    "\n",
    "#before filter out rows that SHIPPED_TO_POSTALCODE is NA\n",
    "print(Fall_2021_reduced_ecomm_final_Item_pos.shape)\n",
    "print(Fall_2021_reduced_ecomm_final_Item_pos.dropna(subset = ['SHIPPED_TO_POSTALCODE']).shape)\n",
    "Fall_2021_reduced_ecomm_final_Item_pos_code = Fall_2021_reduced_ecomm_final_Item_pos.dropna(subset = ['SHIPPED_TO_POSTALCODE'])\n",
    "display(Fall_2021_reduced_web_ecomm_final.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed18555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # figure size in inches\n",
    "rcParams['figure.figsize'] = 11.7,8.27\n",
    "\n",
    "#define a function to find position of a sorted array (from biggest to smallest) based on percentage\n",
    "#so that the values added up to the left of this position \n",
    "def find_cut_point(group, percentage = 100):\n",
    "    item_sorr_arr = group.sort_values(by = 'ITEM_QTY',ascending=False)['ITEM_QTY']\n",
    "    item_sorr_arr_sum = item_sorr_arr.sum()\n",
    "    #the tally of item_quantity up to this point\n",
    "    sum_item = 0\n",
    "    for i, v in item_sorr_arr.items():\n",
    "        sum_item += v\n",
    "        if sum_item>= item_sorr_arr_sum*(percentage/100):\n",
    "            return i\n",
    "\n",
    "#Generate a list of 50 percentile for ITEM_QTY in different provinces\n",
    "percentile_list = []\n",
    "for _, group in groupby_region_style_sum_item.groupby(['SHIPPED_TO_PROVINCE']):\n",
    "    percentile_list.append(find_cut_point(group, 60))\n",
    "\n",
    "#the following is to show the distribution groupby_region_style_sum_item in different province\n",
    "#https://stackoverflow.com/questions/66154773/how-to-add-individual-vlines-to-every-subplot-of-seaborn-facetgrid\n",
    "grid = sns.FacetGrid(groupby_region_style_sum_item, col = \"SHIPPED_TO_PROVINCE\", \n",
    "                     height = 2, col_wrap = 3,sharey = True, sharex = False,\n",
    "                    ylim = (0,50),aspect = 2)\n",
    "grid.map(sns.histplot, \"ITEM_QTY\")\n",
    "grid.set_titles(\"{col_name}\")\n",
    "#add the vertical line in each subplots\n",
    "for ax, pos in zip(grid.axes.flat, groupby_region_style_sum_item.loc[percentile_list].ITEM_QTY.to_numpy()):\n",
    "    ax.axvline(x=pos, color='r', linestyle=':', linewidth = 2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a8c36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function to find position of a sorted array (from biggest to smallest) based on percentage\n",
    "#so that the values added up to the left of this position \n",
    "def find_cut_point(group, percentage = 100):\n",
    "    item_sorr_arr = group.sort_values(by = 'ITEM_QTY',ascending=False)['ITEM_QTY']\n",
    "    item_sorr_arr_sum = item_sorr_arr.sum()\n",
    "    #the tally of item_quantity up to this point\n",
    "    sum_item = 0\n",
    "    for i, v in item_sorr_arr.items():\n",
    "        sum_item += v\n",
    "        if sum_item>= item_sorr_arr_sum*(percentage/100):\n",
    "            return i\n",
    "\n",
    "#Generate a list of 50 percentile for ITEM_QTY in different provinces\n",
    "def style_percentile(percentage = 100):\n",
    "    style_list_prov = defaultdict(list)\n",
    "    info_return_percentage = []\n",
    "    for name, group in groupby_region_style_sum_item.groupby(['SHIPPED_TO_PROVINCE']):\n",
    "        index_percentile = find_cut_point(group, percentage)\n",
    "        #find the list of styles (or quantity) up to this position for a sorted array (from biggest to smallest) for each group\n",
    "        STYLE_NO_sort_arr = group.sort_values(by = 'ITEM_QTY',ascending=False)\n",
    "        #the top style list that makes up the *percentage of the whole list based on the summary of their ITEM_QTY\n",
    "        style_percentile_list = STYLE_NO_sort_arr.loc[:index_percentile]\n",
    "        info_return_percentage.append(style_percentile_list)\n",
    "        style_list_prov[\"Province\"].append(name)\n",
    "        style_list_prov[\"Percentile\"].append(percentage)\n",
    "        style_list_prov[\"total_style\"].append(len(style_percentile_list))\n",
    "#         style_list_prov[\"total_style_list\"].append(style_percentile_list)\n",
    "    return pd.DataFrame(style_list_prov), info_return_percentage\n",
    "# step through percentile from 0 to 100% with 5 % at a time\n",
    "dataframe_percentile = []\n",
    "for perc in range(0, 105, 5):\n",
    "    dataframe_percentile.append(style_percentile(perc)[0])\n",
    "groupby_region_style_sum_item_varying_percentile = pd.concat(dataframe_percentile)\n",
    "# groupby_region_style_sum_item_varying_percentile.to_csv(r\"C:\\Users\\yongpeng.fu\\OneDrive - Canadian Tire\\Desktop\\groupby_region_style_sum_item_varying_percentile.csv\", index = False)\n",
    "groupby_region_style_sum_item_varying_percentile.set_index('Percentile').groupby('Province')['total_style'].plot(legend = True,figsize = (20,6), \n",
    "                                                                                                                 title = \"cut-off percentile vs minumum style list for different province\",\n",
    "                                                                                                                xlabel = 'cut-off percentile',\n",
    "                                                                                                                ylabel = 'minumum style list')\n",
    "plt.axvline(60,color='red', linestyle='--')\n",
    "plt.axhline(1000,color='red', linestyle='--')\n",
    "plt.show()\n",
    "\n",
    "#based on the percentile you chose, write out the excel sheet for those STYLE\n",
    "# pd.concat(style_percentile(60)[1]).to_csv(r\"S:\\Olivia\\Txn Data Extracts from Akshay\\Yongpeng Fu August 9 2022\\groupby_region_style_sum_item_top60_percent_style_list.csv\")\n",
    "\n",
    "# #find the common STYLEs in each different province. Because we know NT, NU and YT only have small purchased quantity,\n",
    "# #I will exclude them from this analysis\n",
    "# groupby_region_style_sum_item_top60_percent_style_list = pd.read_csv(r\"S:\\Olivia\\Txn Data Extracts from Akshay\\Yongpeng Fu August 9 2022\\groupby_region_style_sum_item_top60_percent_style_list.csv\").iloc[:, 1:]\n",
    "# groupby_region_style_sum_item_top60_percent_style_list = groupby_region_style_sum_item_top60_percent_style_list[~groupby_region_style_sum_item_top60_percent_style_list.SHIPPED_TO_PROVINCE.isin([ \"NT\",\"NU\",\"YT\"])]\n",
    "# # province_combinations = combinations(groupby_region_style_sum_item_top60_percent_style_list.SHIPPED_TO_PROVINCE.unique(), 2)\n",
    "# # for i in list(province_combinations):\n",
    "# #     print(i)\n",
    "# common_style = groupby_region_style_sum_item_top60_percent_style_list.query(\"SHIPPED_TO_PROVINCE == 'AB'\").STYLE_NO.values\n",
    "# for _, value in groupby_region_style_sum_item_top60_percent_style_list.groupby('SHIPPED_TO_PROVINCE')['STYLE_NO']:\n",
    "#     common_style = np.intersect1d(common_style, value.values)\n",
    "# pd.DataFrame(common_style).to_csv(r\"C:\\Users\\yongpeng.fu\\OneDrive - Canadian Tire\\Desktop\\common_style.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f80274",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a3ae309",
   "metadata": {},
   "source": [
    "## Summary for What are the top styles in each province so we can increase safety stock…might be top items that cover 50% of the ecomm sales or a list of the top x# of styles\n",
    "\n",
    "1. From the Distribution part, we can see there is skwed distribution for how many units were sold on the style level, which is in our favour because we want to use as few styles as possible to represent as much the purchased quantity as possible. The red vertical line indicates is to split the distribution into 2 parts, to the right part is what are interested most because they are purchased more often than the left part. The red line itself is a percentile that the right part is representing.\n",
    "2. From the \"Find the sweat spot where the minimum number of styles represent the most of the Items purchased\", we can see that the total number of styles needed to fultil the cut-off percentile is growing slow until 80-100%, which is good for us because then we can use minumum styles to cover the majority of the purchased orders. I find 60% is relatively good because all provinces just need less 1000 styles to meet this requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f47092e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbf7e1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
